{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7WszVZTkewdsw8+IOBOvJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayureshpawashe/spark/blob/main/spark_practise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "J4W9yHoO-Pue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Checking Spark Installation & Creating SparkSession"
      ],
      "metadata": {
        "id": "UoAsoD_dZW48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkArchitecture\").getOrCreate()\n",
        "print(\"Spark Version:\", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJCB1u5A-Tha",
        "outputId": "433a0b94-8dd6-4553-bc78-6ca10d20d2a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 3.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating SparkSession (Driver Side)"
      ],
      "metadata": {
        "id": "4cYQfUL0aZgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DriverExample\").getOrCreate()\n",
        "\n",
        "print(\"Driver is running and managing tasks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfVQn7pU_Lys",
        "outputId": "f2f59617-71f2-4b86-cf7f-dcf1ac3d6a6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Driver is running and managing tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running tasks on Executors"
      ],
      "metadata": {
        "id": "6_fknCfTb3BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])  # RDD created\n",
        "squared_rdd = rdd.map(lambda x: x**2)  # Tasks assigned to Executors\n",
        "\n",
        "print(\"RDD processed by Executors:\", squared_rdd.collect())  # Fetch results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qFZ-Ux4BbWf",
        "outputId": "e651cb42-9e22-4fb0-a5a4-eae9574a874c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD processed by Executors: [1, 4, 9, 16, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Demonstrating Parallel Execution"
      ],
      "metadata": {
        "id": "39GQlY_FckDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "rdd = sc.parallelize(range(1, 11), numSlices=2)  # Data is split into 2 partitions\n",
        "tasks = rdd.map(lambda x: (x, x**2))  # Each partition is processed in parallel\n",
        "\n",
        "print(\"Tasks executed on Executors:\", tasks.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_S9y52lBcxB",
        "outputId": "300cba43-02e0-47d4-b110-fc05c940ae72"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasks executed on Executors: [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25), (6, 36), (7, 49), (8, 64), (9, 81), (10, 100)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running Spark in Local Mode"
      ],
      "metadata": {
        "id": "XjCMFjNMeoeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"LocalMode\").getOrCreate()\n",
        "print(\"Spark is running in Local Mode\")\n"
      ],
      "metadata": {
        "id": "gzARrZ84CXIh",
        "outputId": "2c8929df-360a-4fc9-d6f6-ab1d4808f754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark is running in Local Mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating and Processing an RDD"
      ],
      "metadata": {
        "id": "xGudpDG-hMqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Creating an RDD from a Python list\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5, 5])\n",
        "\n",
        "squared_rdd = rdd.map(lambda x: x ** 2)  # Squaring each element\n",
        "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)  # Filtering even numbers\n",
        "mapped_rdd = rdd.map(lambda x: (x, x ** 3))  # Creating key-value pairs (x, x^3)\n",
        "reduced_value = rdd.reduce(lambda x, y: x + y)  # Summing all elements\n",
        "distinct_rdd = rdd.flatMap(lambda x: (x, x)).distinct()  # Duplicating and removing duplicates\n",
        "\n",
        "print(\"RDD elements squared:\", squared_rdd.collect())\n",
        "print(\"Filtered (even numbers):\", filtered_rdd.collect())\n",
        "print(\"Mapped (x, x^3):\", mapped_rdd.collect())\n",
        "print(\"Sum of elements (reduce):\", reduced_value)\n",
        "print(\"Distinct elements (after flatMap and distinct):\", distinct_rdd.collect())\n",
        "\n",
        "\n",
        "#more methods\n",
        "count = rdd.count()  # Counting elements in the RDD\n",
        "first_element = rdd.first()  # Getting the first element\n",
        "rdd_sum = rdd.sum()  # Computing the sum of all elements\n",
        "rdd_max = rdd.max()  # Finding the max element\n",
        "rdd_min = rdd.min()  # Finding the min element\n",
        "\n",
        "print(\"Count of elements:\", count)\n",
        "print(\"First element:\", first_element)\n",
        "print(\"Sum of RDD elements:\", rdd_sum)\n",
        "print(\"Max element:\", rdd_max)\n",
        "print(\"Min element:\", rdd_min)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUkzg4wvXdZA",
        "outputId": "feacec56-f5f6-429c-f44f-7e762bad0036"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD elements squared: [1, 4, 9, 16, 25, 25]\n",
            "Filtered (even numbers): [2, 4]\n",
            "Mapped (x, x^3): [(1, 1), (2, 8), (3, 27), (4, 64), (5, 125), (5, 125)]\n",
            "Sum of elements (reduce): 20\n",
            "Distinct elements (after flatMap and distinct): [2, 4, 1, 3, 5]\n",
            "Count of elements: 6\n",
            "First element: 1\n",
            "Sum of RDD elements: 20\n",
            "Max element: 5\n",
            "Min element: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating and Displaying a DataFrame"
      ],
      "metadata": {
        "id": "gSMEI94xjWO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMXSPchfj40d",
        "outputId": "e3fc1951-9dcb-4969-a7d6-c4bcc9bda405"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 25|\n",
            "|    Bob| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "b-pJuXz3lI1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4qN8NbllJla"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}